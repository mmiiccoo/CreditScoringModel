# -*- coding: utf-8 -*-
"""CreditRatingModelWithScoreComputation

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1naPMuL3mrkOtQCwtHNscSXM2qkKRpwKW

#Import all the packages and dataset needed
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, RepeatedStratifiedKFold, cross_val_score
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import roc_curve, roc_auc_score, confusion_matrix, precision_recall_curve, auc
from sklearn.feature_selection import f_classif
from sklearn.pipeline import Pipeline
from sklearn.base import BaseEstimator, TransformerMixin
from scipy.stats import chi2_contingency
from category_encoders import WOEEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.metrics import accuracy_score
 


loan_data = pd.read_csv('data.csv')

pd.options.display.max_columns = None
loan_data.head()

loan_data.info()

"""Identify the Target Variable"""

loan_data['Defaulter'].value_counts(normalize = True)

# create a new column based on the loan_status column that will be our target variable
loan_data['good_bad'] = np.where(loan_data.loc[:, 'Defaulter'].isin(['yes']), 0, 1)
# Drop the original 'loan_status' column
loan_data.drop(columns = ['Defaulter'], inplace = True)

print(loan_data['good_bad'])

"""#Split the Data"""

# split data into 80/20 while keeping the distribution of bad loans in test set same as that in the pre-split dataset
X = loan_data.drop('good_bad', axis = 1)
y = loan_data['good_bad']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42, stratify = y)

# specifically hard copying the training sets to avoid Pandas' SetttingWithCopyWarning when we play around with this data later on.
X_train, X_test = X_train.copy(), X_test.copy()

"""#General Data Cleaning"""

X_train.info()

"""#Feature Selection"""

# first divide training data into categorical and numerical subsets
X_train_cat = X_train.select_dtypes(include = 'object').copy()

# define an empty dictionary to store chi-squared test results
chi2_check = {}

# loop over each column in the training set to calculate chi-statistic with the target variable
for column in X_train_cat:
    chi, p, dof, ex = chi2_contingency(pd.crosstab(y_train, X_train_cat[column]))
    chi2_check.setdefault('Feature',[]).append(column)
    chi2_check.setdefault('p-value',[]).append(round(p, 10))

# convert the dictionary to a DF
chi2_result = pd.DataFrame(data = chi2_check)
chi2_result.sort_values(by = ['p-value'], ascending = True, ignore_index = True, inplace = True)
chi2_result

X_train.info()

# reindex the dummied test set variables to make sure all the feature columns in the train set are also available in the test set
X_test = X_test.reindex(labels=X_train.columns, axis=1, fill_value=0)

"""WOE Binning

"""

# Create copies of the 4 training sets to be preprocessed using WoE
X_train_prepr = X_train.copy()
y_train_prepr = y_train.copy()
X_test_prepr = X_test.copy()
y_test_prepr = y_test.copy()

X_train_prepr.info()

# The function takes 3 arguments: a dataframe (X_train_prepr), a string (column name), and a dataframe (y_train_prepr).
# The function returns a dataframe as a result.
def woe_discrete(df, cat_variable_name, y_df, x_df_train, x_df_test):
    df = pd.concat([df[cat_variable_name], y_df], axis = 1)
    df = pd.concat([df.groupby(df.columns.values[0], as_index = False)[df.columns.values[1]].count(),
                    df.groupby(df.columns.values[0], as_index = False)[df.columns.values[1]].mean()], axis = 1)
    df = df.iloc[:, [0, 1, 3]]
    df.columns = [df.columns.values[0], 'counts', 'prop_good']

    df['number_of_good'] = df['prop_good'] * df['counts']
    df['number_of_bad'] = (1 - df['prop_good']) * df['counts']

    df['prop_n_good'] = df['number_of_good'] / df['number_of_good'].sum()
    df['prop_n_bad'] = df['number_of_bad'] / df['number_of_bad'].sum()

    df['WoE'] = np.log(df['prop_n_good'] / df['prop_n_bad'])
    df = df.sort_values(['WoE'])
    df = df.reset_index(drop = True)

    df['IV'] = (df['prop_n_good'] - df['prop_n_bad']) * df['WoE']

    mask = np.isfinite(df['IV'])

    iv_sum = df.loc[mask,'IV'].sum()

    df['Total IV'] = iv_sum

    # Check if iv_sum is less than 0.02
    if iv_sum < 0.02:
        # Drop the cat_variable_name column from df and y_df
        # Check if iv_sum is less than 0.02
      x_df_train.drop(cat_variable_name, axis=1, inplace=True)
      x_df_test.drop(cat_variable_name, axis=1, inplace=True)


    return df

X_test.info()

# We set the default style of the graphs to the seaborn style.
sns.set()
# Below we define a function for plotting WoE across categories that takes 2 arguments: a dataframe and a number.
def plot_by_woe(df_WoE, rotation_of_x_axis_labels = 0):
    x = np.array(df_WoE.iloc[:, 0].apply(str))
    y = df_WoE['WoE']
    plt.figure(figsize=(18, 6))
    plt.plot(x, y, marker = 'o', linestyle = '--', color = 'k')
    plt.xlabel(df_WoE.columns[0])
    plt.ylabel('Weight of Evidence')
    plt.title(str('Weight of Evidence by ' + df_WoE.columns[0]))
    plt.xticks(rotation = rotation_of_x_axis_labels)

"""Sex WOE and IV Computation"""

df_temp = woe_discrete(X_train_prepr, 'Sex', y_train_prepr, X_train, X_test)
df_temp

"""Marital Status WOE and IV Computation"""

df_temp = woe_discrete(X_train_prepr, 'Marital Status', y_train_prepr, X_train, X_test)
df_temp

"""Length of current employment WOE and IV"""

df_temp = woe_discrete(X_train_prepr, 'Residence', y_train_prepr, X_train, X_test)
df_temp

"""Current Housing Siutation"""

df_temp = woe_discrete(X_train_prepr, 'Current Housing Situation', y_train_prepr, X_train, X_test)
df_temp

"""Employment Status WOE and IV Computation"""

df_temp = woe_discrete(X_train_prepr, 'Employment Status', y_train_prepr, X_train, X_test)
df_temp

"""Job Industry WOE and IV Computation"""

df_temp = woe_discrete(X_train_prepr, 'Job Industry', y_train_prepr, X_train, X_test)
df_temp

"""Job Industry WOE and IV Computation"""

df_temp = woe_discrete(X_train_prepr, 'Loan History', y_train_prepr, X_train, X_test)
df_temp

"""Real Estate / Home / Car / Motorcycle Stocks / Insurance Policy / Jewelry / Precious Metals"""

df_temp = woe_discrete(X_train_prepr, 'Car', y_train_prepr, X_train, X_test)
df_temp

df_temp = woe_discrete(X_train_prepr, 'Stocks', y_train_prepr, X_train, X_test)
df_temp

df_temp = woe_discrete(X_train_prepr, 'Real estate / Home', y_train_prepr, X_train, X_test)
df_temp

df_temp = woe_discrete(X_train_prepr, 'Motorcycle', y_train_prepr, X_train, X_test)
df_temp

df_temp = woe_discrete(X_train_prepr, 'Jewelry / Precious Metals', y_train_prepr, X_train, X_test)
df_temp

df_temp = woe_discrete(X_train_prepr, 'Insurance Policy', y_train_prepr, X_train, X_test)
df_temp

"""#WOE and IV Compuation of Numerical Features

Age WOE and IV Computation
"""

df_temp = woe_discrete(X_train_prepr, 'Age', y_train_prepr, X_train, X_test)
df_temp

"""Number of Dependents WOE and IV Computation"""

df_temp = woe_discrete(X_train_prepr, 'Number of Dependents', y_train_prepr, X_train, X_test)
df_temp

"""Years in Residence WOE and IV Computation"""

df_temp = woe_discrete(X_train_prepr, 'Years in Residence', y_train_prepr, X_train, X_test)
df_temp

"""Monthly Income / Monthly Expenses / Current Savings WOE and IV Compuation"""

df_temp = woe_discrete(X_train_prepr, 'Current Savings', y_train_prepr, X_train, X_test)
df_temp

df_temp = woe_discrete(X_train_prepr, 'Monthly Income', y_train_prepr, X_train, X_test)
df_temp

df_temp = woe_discrete(X_train_prepr, 'Monthly Expenses', y_train_prepr, X_train, X_test)
df_temp

df_temp = woe_discrete(X_train_prepr, 'Highest Educational Attainment', y_train_prepr, X_train, X_test)
df_temp

X_train.info()

"""# Define Custom Class for Rebinning // not needed since our data is not that complex and already defined

#Reconfirm shape of the 4 datasets
"""

print(X_train.shape)
print(y_train.shape)
print(X_test.shape)
print(y_test.shape)



# define the columns to be preprocessed
cat_cols = list(X_train.columns)
# define the preprocessing pipelines for the numerical and categorical columns
cat_transformer = Pipeline(steps=[('onehot', OneHotEncoder(handle_unknown='ignore'))])

# combine the preprocessing pipelines using a ColumnTransformer
preprocessor = ColumnTransformer(transformers=[('cat', cat_transformer, cat_cols)])

# define the modeling pipeline
model = LogisticRegression(max_iter=1000, class_weight='balanced')
pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('model', model)])

# fit and evaluate the pipeline on the training data
pipeline.fit(X_train, y_train)
pipeline.fit(X_test, y_test)

print(X_train.shape)
print(y_train.shape)
print(X_test.shape)
print(y_test.shape)


# make predictions on the test set
y_pred = pipeline.predict(X_test)

# calculate the accuracy of the predictions
accuracy = accuracy_score(y_test, y_pred)

# print the accuracy
print('Accuracy: %.4f' % (accuracy))

# pretty impressive scores for the first time, now let's fit the pipeline on the whole training set
pipeline.fit(X_train, y_train)

# get the names of the one-hot encoded categorical features
cat_feature_names = pipeline.named_steps['preprocessor'].named_transformers_['cat'].named_steps['onehot'].get_feature_names_out(cat_cols)

# combine the numerical and categorical feature names
feature_names = np.concatenate([cat_feature_names])

# create a summary table with the feature names
summary_table = pd.DataFrame({'Feature name': feature_names})

# add the coefficients to the summary table
summary_table['Coefficients'] = pipeline.named_steps['model'].coef_[0]

# add the intercept to the summary table
summary_table.loc[-1] = ['Intercept', pipeline.named_steps['model'].intercept_[0]]
summary_table.index = summary_table.index + 1
summary_table.sort_index(inplace=True)

# display the summary table
summary_table

# make preditions on our test set
y_hat_test = pipeline.predict(X_test)
# get the predicted probabilities
y_hat_test_proba = pipeline.predict_proba(X_test)
# select the probabilities of only the positive class (class 1 - default)
y_hat_test_proba = y_hat_test_proba[:][: , 1]

# we will now create a new DF with actual classes and the predicted probabilities
# create a temp y_test DF to reset its index to allow proper concaternation with y_hat_test_proba
y_test_temp = y_test.copy()
y_test_temp.reset_index(drop = True, inplace = True)
y_test_proba = pd.concat([y_test_temp, pd.DataFrame(y_hat_test_proba)], axis = 1)
# check the shape to make sure the number of rows is same as that in y_test
y_test_proba.shape

# Rename the columns
y_test_proba.columns = ['y_test_class_actual', 'y_hat_test_proba']
# Makes the index of one dataframe equal to the index of another dataframe.
y_test_proba.index = X_test.index
y_test_proba

# assign a threshold value to differentiate good with bad
tr = 0.5
# crate a new column for the predicted class based on predicted probabilities and threshold
# We will determine this optimat threshold later in this project
y_test_proba['y_test_class_predicted'] = np.where(y_test_proba['y_hat_test_proba'] > tr, 1, 0)
# create the confusion matrix
confusion_matrix(y_test_proba['y_test_class_actual'], y_test_proba['y_test_class_predicted'], normalize = 'all')

# get the values required to plot a ROC curve
fpr, tpr, thresholds = roc_curve(y_test_proba['y_test_class_actual'], y_test_proba['y_hat_test_proba'])
# plot the ROC curve
plt.plot(fpr, tpr)
# plot a secondary diagonal line, with dashed line style and black color to represent a no-skill classifier
plt.plot(fpr, fpr, linestyle = '--', color = 'k')
plt.xlabel('False positive rate')
plt.ylabel('True positive rate')
plt.title('ROC curve');

# Calculate the Area Under the Receiver Operating Characteristic Curve (AUROC) on our test set
AUROC = roc_auc_score(y_test_proba['y_test_class_actual'], y_test_proba['y_hat_test_proba'])
AUROC

# calculate Gini from AUROC
Gini = AUROC * 2 - 1
Gini

# draw a PR curve
# calculate the no skill line as the proportion of the positive class
no_skill = len(y_test[y_test == 1]) / len(y)
# plot the no skill precision-recall curve
plt.plot([0, 1], [no_skill, no_skill], linestyle='--', label='No Skill')

# calculate inputs for the PR curve
precision, recall, thresholds = precision_recall_curve(y_test_proba['y_test_class_actual'], y_test_proba['y_hat_test_proba'])
# plot PR curve
plt.plot(recall, precision, marker='.', label='Logistic')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.legend()
plt.title('PR curve');

# calculate PR AUC
auc_pr = auc(recall, precision)
auc_pr

summary_table

ref_categories = ['Age:>85', 'Sex:Male', 'Number of Dependents:>5', 'Highest Educational Attainment:College', 'Current Housing Situation:Own',
		  'Years in Residence:>10', 'Employment Status:Employed full-time', 'Job Industry:Banking and Finance', 'Monthly Income:>10000',
		  'Monthly Expenses:>10000', 'Current Savings:>100000', 'Car:no', 'Insurance Policy:no']

# We create a new dataframe with one column. Its values are the values from the 'reference_categories' list. We name it 'Feature name'.
df_ref_categories = pd.DataFrame(ref_categories, columns = ['Feature name'])
# We create a second column, called 'Coefficients', which contains only 0 values.
df_ref_categories['Coefficients'] = 0
df_ref_categories

# Concatenates two dataframes.
df_scorecard = pd.concat([summary_table, df_ref_categories])
# We reset the index of a dataframe.
df_scorecard.reset_index(inplace = True)

 # create a boolean mask to filter out rows with 0 values in the 'Coefficients' column
mask = df_scorecard['Coefficients'] != 0

 # apply the mask to the DataFrame to keep only the rows with non-zero values in the 'Coefficients' column
df_scorecard = df_scorecard[mask]

df_scorecard

# create a new column, called 'Original feature name', which contains the value of the 'Feature name' column, up to the column symbol.
df_scorecard['Original feature name'] = df_scorecard['Feature name'].str.split('_').str[0]
df_scorecard

# Define the min and max threshholds for our scorecard
min_score = 300
max_score = 850

# recalculate the sum of the minimum and maximum coefficients of each category within the original feature name
min_sum_coef = df_scorecard.groupby('Original feature name')['Coefficients'].min().sum()
max_sum_coef = df_scorecard.groupby('Original feature name')['Coefficients'].max().sum()

# recalculate the score based on the updated DataFrame
df_scorecard['Score - Calculation'] = df_scorecard['Coefficients'] * (max_score - min_score) / (max_sum_coef - min_sum_coef)
df_scorecard.loc[0, 'Score - Calculation'] = ((df_scorecard.loc[0,'Coefficients'] - min_sum_coef) / (max_sum_coef - min_sum_coef)) * (max_score - min_score) + min_score
df_scorecard['Score - Preliminary'] = df_scorecard['Score - Calculation'].round()
df_scorecard

# check the min and max possible scores of our scorecard
min_sum_score_prel = df_scorecard.groupby('Original feature name')['Score - Preliminary'].min().sum()
max_sum_score_prel = df_scorecard.groupby('Original feature name')['Score - Preliminary'].max().sum()
print(min_sum_score_prel)
print(max_sum_score_prel)

# so both our min and max scores are out by +1. we need to manually adjust this
# Which one? We'll evaluate based on the rounding differences of the minimum category within each Original Feature Name.
pd.options.display.max_rows = 102
df_scorecard['Difference'] = df_scorecard['Score - Preliminary'] - df_scorecard['Score - Calculation']
df_scorecard

# look like we can get by deducting 1 from the Intercept
df_scorecard['Score - Final'] = df_scorecard['Score - Preliminary']
df_scorecard.loc[0, 'Score - Final'] = 607
df_scorecard

df_scorecard.to_csv('scorecard.csv', index='False')

# Recheck min and max possible scores
print(df_scorecard.groupby('Original feature name')['Score - Final'].min().sum())
print(df_scorecard.groupby('Original feature name')['Score - Final'].max().sum())

X_test.head()

categorical_columns = X_test.select_dtypes(include=['object']).columns

X_test_encoded = pd.get_dummies(X_test, columns=categorical_columns)


X_test_encoded.head()

categorical_columns

column_label = X_test_encoded.columns[37]

X_test_encoded = X_test_encoded.drop(column_label, axis=1)

X_test_encoded.info()

X_test_encoded.head()

# get the list of our final scorecard scores
df_scorecard_list = pd.read_csv('scorelist.csv')
# Select the 'Score - Final' column
scorecard_scores = df_scorecard_list['Score - Final']
# check the shapes of test set and scorecard before doing matrix dot multiplication
print(X_test_encoded.shape)
print(scorecard_scores.shape)

X_test_encoded.insert(0, 'Intercept', 1)
X_test_encoded.head()

# check the shapes of test set and scorecard before doing matrix dot multiplication
print(X_test_encoded.shape)
print(scorecard_scores.shape)

# Need to reshape scorecard_scores so that it is (102,1) to allow for matrix dot multiplication
scorecard_scores = scorecard_scores.values.reshape(47, 1)
print(X_test_encoded.shape)
print(scorecard_scores.shape)

X_test_encoded.head()

# matrix dot multiplication of test set with scorecard scores
y_scores = X_test_encoded.dot(scorecard_scores)
y_scores

y_scores.info()

y_scores = y_scores.rename(columns = {0: 'Score'}, inplace = False)

y_scores.sort_values(by='Score')

y_scores.to_csv('X_test_scores.csv')

X_train.info()

categorical_columns = X_train.select_dtypes(include=['object']).columns

X_train_encoded = pd.get_dummies(X_train, columns=categorical_columns)


X_train_encoded.head()

X_train_encoded.info()

# get the list of our final scorecard scores
df_scorecard_list_X_train = pd.read_csv('scorelist_X_train.csv')
# Select the 'Score - Final' column
scorecard_scores_X_train = df_scorecard_list_X_train['Score - Final']
# check the shapes of test set and scorecard before doing matrix dot multiplication
print(X_train_encoded.shape)
print(scorecard_scores_X_train.shape)

scorecard_scores_X_train = scorecard_scores_X_train .values.reshape(56, 1)

# check the shapes of test set and scorecard before doing matrix dot multiplication
print(X_train_encoded.shape)
print(scorecard_scores_X_train.shape)

X_train_encoded.insert(0, 'Intercept', 1)

X_train_encoded.head()

X_train_encoded.to_csv('X_train_encoded_modified.csv')

# matrix dot multiplication of test set with scorecard scores
y_scores = X_train_encoded.dot(scorecard_scores_X_train)
y_scores

# Commented out IPython magic to ensure Python compatibility.
# %pwd

y_scores.info()
y_scores = y_scores.rename(columns = {0: 'Score'}, inplace = False)
y_scores.sort_values(by='Score')
y_scores.to_csv('X_train_scores.csv')

